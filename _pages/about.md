---
permalink: /
title: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

<h3 style="padding-top: 0; margin-top: 0; 0; margin-bottom: 0.5em; font-size: 1.1em; color: #094480">About Me</h3>
I am a computer science PhD candidate at the [Courant Institute of Mathematical Sciences](https://cims.nyu.edu/) of [New York University](https://nyu.edu/), with an affiliation to [NYU Shanghai](https://shanghai.nyu.edu/). I am also a visiting scholar in the Machine Learning Department at [MBZUAI](https://mbzuai.ac.ae/). I conduct research under the advice of [Prof. Gus Xia](http://www.musicxlab.com/members/gus/) in [Music X Lab](http://www.musicxlab.com). In 2019, I received my undergraduate degree in mathematics at [Fudan University](https://www.fudan.edu.cn/en/). I am also a conductor, pianist, and *Erhu* (Chinese string instrument) player. I was formerly the conductor of NYU Shanghai Jazz Ensemble and director of the Fudan Musical Club.

<h3 style="padding-top: 0; margin-top: 0; 0; margin-bottom: 0.5em; font-size: 1.1em; color: #094480">Research Interest</h3>


The primary goal of my research is to _make intelligent systems more human-like_, and I find music to be one of the rich domains to explore this question in depth. My research is highly cross-disciplinary, combining elements of _Computer music_, _Generative AI_, _Representation Learning_, _Interpretability_, _Style Transfer_, and _Human-Computer Interaction_.

Particularly, I design generative models that learn to utilize the hierarchical and nuanced music concepts in music creation, and I develop methods that enable machines to emerge human-interpretable symbolic languages and concepts. I also leverage these methods to design controllable and interactive applications for music creation and education. I hope that through research, we understand ourselves better and we create a more intuitive human-machine co-creation experience.

Here is a brief overview of my research, organized by topic:

<div class="project">
  <div class="project-image">
    <img src="assets/img/whole-song.png" alt="Project Image">
  </div>
  <div class="project-content">
    <h3>Hierarchical Music Generation & Arrangement</h3>
    <ul>
    <li>Whole-song generation via <strong>compositional hierarchy</strong></li>
    <li style="list-style-type: none;">
      [<a href="https://openreview.net/forum?id=sn7CYWyavh" target="_blank">WholeSongGen</a>]
    </li>
    <li>Modeling long-term context dependency</li>
      <li style="list-style-type: none;">
      [<a href="https://nips.cc/virtual/2024/poster/95545" target="_blank">AccomontageIII</a>][<a href="https://ieeexplore.ieee.org/document/10096446" target="_blank">Mixed-level Inpainting</a>]
    </li>
    <li>Dataset</li>
      <li style="list-style-type: none;">
      [<a href="https://archives.ismir.net/ismir2020/paper/000089.pdf" target="_blank">POP909</a>]
    </li>
    </ul>
  </div>
</div>

<div class="project">
  <div class="project-image">
    <img src="assets/img/repr-dis1.png" alt="Project Image">
  </div>
  <div class="project-content">
    <h3>Representation Learning</h3>
    <ul>
    <li>Representation Disentanglement: monophonic & polyphonic</li>
    <li style="list-style-type: none;">
      [<a href="https://archives.ismir.net/ismir2019/paper/000072.pdf" target="_blank">EC2-VAE</a>][<a href="https://archives.ismir.net/ismir2020/paper/000094.pdf" target="_blank">Poly-Dis</a>]
    </li>
    <li>Cross-modality representation: audio & text</li>
      <li style="list-style-type: none;">
      [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9747884" target="_blank">Audio2Midi</a>][<a href="https://aclanthology.org/2020.nlp4musa-1.11.pdf" target="_blank">BUTTER</a>]
    </li>
    </ul>
  </div>
</div>


<div class="project">
  <div class="project-image">
    <img src="assets/img/v3.png" alt="Project Image">
  </div>
  <div class="project-content">
    <h3>Unsupervised Concept Emergence</h3>
    <ul>
    <li>Learning content & style via variability constraints</li>
    <li style="list-style-type: none;">
      [<a href="https://arxiv.org/abs/2407.03824" target="_blank">V3</a>]
    </li>
    <li>Unsupervised modeling of music structure</li>
      <li style="list-style-type: none;">
      [<a href="https://github.com/ZZWaang/PianoTree-VAE" target="_blank">PianoTree-VAE</a>][<a href="https://archives.ismir.net/ismir2021/paper/000090.pdf" target="_blank">MuseBERT</a>]
    </li>
    </ul>
  </div>
</div>


<div class="project">
  <div class="project-image">
    <img style="width: 100%;  margin-left: 0%;  margin-right: 0%;" src="assets/img/me-and-teo.png" alt="Project Image">
  </div>
  <div class="project-content">
    <h3>Music Co-creation & Education</h3>
    <ul>
    <li>Controllable Music Generation</li>
    <li style="list-style-type: none;">
      [<a href="https://www.youtube.com/watch?v=jjr2gDgzrMg" target="_blank">Teo1</a>][<a href="https://youtu.be/YmGdjIdHEQ4" target="_blank">Teo2</a>][<a href="https://youtu.be/bteuPnpHNx8" target="_blank">Teo3</a>][<a href="https://www.youtube.com/watch?v=Ahfk0hvpCzM&t=3316s" target="_blank">Live</a>]
    </li>
    <li>Music Education Interface</li>
      <li style="list-style-type: none;">
      [<a href="https://www.youtube.com/watch?v=Ahfk0hvpCzM&t=3163s" target="_blank">Live</a>][<a href="link-to-paper" target="_blank">StyleWheel</a>]
    </li>
    </ul>
  </div>
</div>
